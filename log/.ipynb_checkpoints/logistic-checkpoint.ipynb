{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXITER = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare our logisitic regression to `SGDClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.1, learning_rate='constant', loss='log', max_iter=100,\n",
       "              penalty='none', shuffle=False, tol=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyX = pd.read_csv('data/simple_data.csv') \n",
    "toyY = toyX.pop(\"stroke\") #remove target column\n",
    "\n",
    "alpha = .1 #learning rate\n",
    "\n",
    "sgd = SGDClassifier(loss='log', max_iter=MAXITER,shuffle=False, tol=None, penalty='none', learning_rate='constant', eta0 = alpha)\n",
    "sgd.fit(toyX,toyY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printWeights(features, weights):\n",
    "    \"\"\"\n",
    "    print model weights\n",
    "    \"\"\"\n",
    "    if \"bias\" not in features:\n",
    "        features = list(features)+[\"bias\"]\n",
    "    print(\"\\t%30s %10s\" % (\"Feature\", \"Weight\"))\n",
    "    for i in range(len(features)):\n",
    "        print(\"\\t%30s %10.3f\" % (features[i], weights[i]))\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    normalize X to range 0-1\n",
    "    \"\"\"\n",
    "    X = X - X.min() # start at 0\n",
    "    normalizedX = X/X.max() # set max to 1\n",
    "    return normalizedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, features, labels, alpha, lmbda = 0):\n",
    "        self.alpha = alpha #learning rate\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.lmbda = lmbda#regularization term\n",
    "        self.labelToNumberDict = {}\n",
    "        for i in range(len(labels)):#are unique\n",
    "            self.labelToNumberDict[labels[i]] = i \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(toyX.columns, toyY.unique(),alpha) # create a model similar to the instance SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression):\n",
    "    \n",
    "    def loss(self, xi):\n",
    "        npWeights = self.weights#.to_numpy()\n",
    "        npXi = xi.to_numpy()\n",
    "        return 1/(1+np.exp(-1*np.dot(npWeights, npXi)))\n",
    "    \n",
    "    def fit(self, Xinput, Y):\n",
    "        \"\"\"\n",
    "        train model weights via gradient descent\n",
    "        \"\"\"\n",
    "        X = Xinput.copy()\n",
    "        X[\"bias\"] = 1#add a bias\n",
    "        self.weights = np.zeros(len(X.columns))\n",
    "        self.w = self.weights\n",
    "        for z in range(MAXITER):\n",
    "            gradient = np.zeros(len(X.columns))\n",
    "            for index, xi in X.iterrows(): #might be problem\n",
    "                p = self.loss(xi)\n",
    "                yi = self.labelToNumberDict[Y[index]]\n",
    "                error = p - yi\n",
    "\n",
    "                for j in range(len(X.columns)):\n",
    "                  gradient[j] = error*xi.iloc[j]\n",
    "                self.weights -= alpha*gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients and intercept for SKlearn SGDClassifier: \n",
      "\t                       Feature     Weight\n",
      "\t                           age      1.967\n",
      "\t                    cholestrol     -1.888\n",
      "\t                          bias     -4.422\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "\t                       Feature     Weight\n",
      "\t                           age      1.967\n",
      "\t                    cholestrol     -1.888\n",
      "\t                          bias     -4.422\n"
     ]
    }
   ],
   "source": [
    "lr =  LogisticRegression(toyX.columns, toyY.unique(),alpha) # create our model\n",
    "lr.fit(toyX,toyY) #fit the same data as the SGDClassifier\n",
    "\n",
    "print(\"Coefficients and intercept for SKlearn SGDClassifier: \")\n",
    "weights = list(sgd.coef_[0]) + list(sgd.intercept_) \n",
    "printWeights(toyX.columns,  weights)\n",
    "\n",
    "print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "printWeights(toyX.columns,  lr.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression):\n",
    "    \n",
    "    def predict(self, Xinput):\n",
    "        \"\"\"\n",
    "        returns array of predicted labels for each exmple in Xinput\n",
    "        \"\"\"\n",
    "        X = Xinput.copy()\n",
    "        X[\"bias\"] = 1#add a bias\n",
    "        predictions = []\n",
    "        #print(f\"X type: {type(X)}, X: {X}\")\n",
    "        for index, xi in X.iterrows():\n",
    "            predictions.append(self.labels[int(np.round(self.loss(xi)))])\n",
    "        return predictions\n",
    "\n",
    "    def predict_prob(self, Xinput):\n",
    "        \"\"\"\n",
    "        returns a 2D array of size [# examples, # classes] containing the probability \n",
    "        of each prediction.  \n",
    "        \"\"\"\n",
    "        X = Xinput.copy()\n",
    "        X[\"bias\"] = 1#add a bias\n",
    "        predictions = []\n",
    "        for index, xi in X.iterrows():\n",
    "            loss = self.loss(xi)\n",
    "            temp = []\n",
    "            temp.append(loss)\n",
    "            temp.append(abs(1-loss))\n",
    "            predictions.append(temp)\n",
    "        return predictions\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def print_errors(self, X_test, y_test, predictions, num_errors):\n",
    "        print(str(num_errors) + \" Errors: \")\n",
    "        for i in range(len(predictions)):\n",
    "            if num_errors == 0:\n",
    "                break\n",
    "            correct = y_test.iloc[i]\n",
    "            if predictions[i] != correct:\n",
    "                print(\"X_test: \", X_test.iloc[i], \"\\n prediction: \", predictions[i], \"actual: \", y_test.iloc[i])\n",
    "                print('\\n')\n",
    "                num_errors -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and Predictions for SKlearn SGDClassifier: \n",
      "Accuracy: 100.00%\n",
      "  Prediction        Truth\n",
      "-------------------------\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "\n",
      "Accuracy and Predictions for Logistic Regression: \n",
      "Accuracy: 100.00%\n",
      "  Prediction        Truth\n",
      "-------------------------\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n",
      "         Pos          Pos\n",
      "         Neg          Neg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0850105415875708, 0.9149894584124292],\n",
       " [0.6487723414548504, 0.3512276585451496],\n",
       " [0.39909478000660176, 0.6009052199933982],\n",
       " [0.8202769035004075, 0.1797230964995925],\n",
       " [0.19276798260874362, 0.8072320173912564],\n",
       " [0.9890997470995166, 0.010900252900483354],\n",
       " [0.012830250993056852, 0.9871697490069431]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy and Predictions for SKlearn SGDClassifier: \")\n",
    "print(\"Accuracy: %0.2f%%\" % (sgd.score(toyX,toyY)*100))\n",
    "predictions = sgd.predict(toyX)\n",
    "print(\"%12s %12s\" % (\"Prediction\", \"Truth\"))\n",
    "print(\"-\"*25)\n",
    "for i in range(len(predictions)):\n",
    "    print(\"%12s %12s\" % (predictions[i], toyY[i]))\n",
    "print()\n",
    "\n",
    "lr = LogisticRegression(toyX.columns, toyY.unique(),alpha) # create a model similar to the SGDClassifier above\n",
    "lr.fit(toyX,toyY) #fit the same data as the SGDClassifier\n",
    "print(\"Accuracy and Predictions for Logistic Regression: \")\n",
    "print(\"Accuracy: %0.2f%%\" % (len(toyY[toyY == lr.predict(toyX)])/len(toyY)*100))\n",
    "predictions = lr.predict(toyX)\n",
    "print(\"%12s %12s\" % (\"Prediction\", \"Truth\"))\n",
    "print(\"-\"*25)\n",
    "for i in range(len(predictions)):\n",
    "    print(\"%12s %12s\" % (predictions[i], toyY[i]))\n",
    "\n",
    "lr.predict_prob(toyX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with mammal_train.csv and mammal_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression):\n",
    "    def score(self, X_test, y_test):#score by accuracy: # correct / total predictions\n",
    "        predictions = self.predict(X_test)\n",
    "        numCorrect = 0\n",
    "        for i in range(len(predictions)):\n",
    "            correct = y_test.iloc[i]\n",
    "            if predictions[i] == correct:\n",
    "                numCorrect += 1\n",
    "        return float(numCorrect)/float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and Predictions for Logistic Regression: \n",
      "Accuracy: 100.00%\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "\t                       Feature     Weight\n",
      "\t                          hair      2.926\n",
      "\t                      feathers     -1.355\n",
      "\t                          eggs     -3.682\n",
      "\t                          milk      4.330\n",
      "\t                      airborne     -1.426\n",
      "\t                       aquatic     -0.601\n",
      "\t                      predator     -0.649\n",
      "\t                       toothed      0.361\n",
      "\t                      backbone      0.441\n",
      "\t                      breathes      0.484\n",
      "\t                      venomous     -1.511\n",
      "\t                          fins     -0.574\n",
      "\t                          legs     -0.523\n",
      "\t                          tail     -0.216\n",
      "\t                      domestic     -0.149\n",
      "\t                       catsize      1.999\n",
      "\t                          bias     -1.804\n"
     ]
    }
   ],
   "source": [
    "mammal_train = pd.read_csv('data/mammal_train.csv')\n",
    "mammal_test = pd.read_csv('data/mammal_test.csv')\n",
    "\n",
    "# shuffle the examples \n",
    "mammal_train = mammal_train.sample(frac=1).reset_index(drop=True)\n",
    "mammal_test = mammal_test.sample(frac=1).reset_index(drop=True)\n",
    "mammal_train[\"legs\"] /= 8 # normalize legs so between 0 and 1\n",
    "mammal_test[\"legs\"] /= 8 # normalize legs so between 0 and 1\n",
    "\n",
    "ytrain = mammal_train.pop(\"animalType\")\n",
    "ytest = mammal_test.pop(\"animalType\")\n",
    "\n",
    "mammalLR = LogisticRegression(mammal_train.columns, ytrain.unique(), alpha)\n",
    "mammalLR.fit(mammal_train, ytrain)\n",
    "mammalPred = mammalLR.predict(mammal_test)\n",
    "\n",
    "print(\"Accuracy and Predictions for Logistic Regression: \")\n",
    "print(\"Accuracy: %0.2f%%\" % (mammalLR.score(mammal_test, ytest)*100))\n",
    "\n",
    "print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "printWeights(mammal_train.columns,  mammalLR.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with titantic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Results for alpha =  0====\n",
      "Accuracy and Predictions for Logistic Regression: \n",
      "Accuracy: 61.62%\n",
      "10 Errors: \n",
      "X_test:  Pclass    0.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.000000\n",
      "Fare      0.139136\n",
      "Name: 1, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.015469\n",
      "Name: 2, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.000000\n",
      "Fare      0.103644\n",
      "Name: 3, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.333333\n",
      "Fare      0.021731\n",
      "Name: 8, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.500000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.000000\n",
      "Fare      0.058694\n",
      "Name: 9, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.166667\n",
      "Fare      0.032596\n",
      "Name: 10, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.051822\n",
      "Name: 11, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.50000\n",
      "Sex       1.00000\n",
      "SibSp     0.00000\n",
      "Parch     0.00000\n",
      "Fare      0.03123\n",
      "Name: 15, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.500000\n",
      "Sex       0.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.025374\n",
      "Name: 17, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.014102\n",
      "Name: 19, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "\t                       Feature     Weight\n",
      "\t                        Pclass      0.000\n",
      "\t                           Sex      0.000\n",
      "\t                         SibSp      0.000\n",
      "\t                         Parch      0.000\n",
      "\t                          Fare      0.000\n",
      "\t                          bias      0.000\n",
      "==== Results for alpha =  1====\n",
      "Accuracy and Predictions for Logistic Regression: \n",
      "Accuracy: 78.00%\n",
      "10 Errors: \n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.015469\n",
      "Name: 2, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.333333\n",
      "Fare      0.021731\n",
      "Name: 8, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.166667\n",
      "Fare      0.032596\n",
      "Name: 10, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.500000\n",
      "Sex       0.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.025374\n",
      "Name: 17, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.014102\n",
      "Name: 19, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.500000\n",
      "Sex       0.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.025374\n",
      "Name: 21, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.015672\n",
      "Name: 22, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    0.000000\n",
      "Sex       0.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.069291\n",
      "Name: 23, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.125000\n",
      "Parch     0.833333\n",
      "Fare      0.061264\n",
      "Name: 25, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "X_test:  Pclass    1.000000\n",
      "Sex       1.000000\n",
      "SibSp     0.000000\n",
      "Parch     0.000000\n",
      "Fare      0.015379\n",
      "Name: 28, dtype: float64 \n",
      " prediction:  0 actual:  1\n",
      "\n",
      "\n",
      "Coefficients and intercept for Logistic Regression: \n",
      "\t                       Feature     Weight\n",
      "\t                        Pclass     -3.472\n",
      "\t                           Sex      3.086\n",
      "\t                         SibSp     -2.915\n",
      "\t                         Parch      0.374\n",
      "\t                          Fare      2.364\n",
      "\t                          bias     -1.028\n"
     ]
    }
   ],
   "source": [
    "titanicX  = pd.read_csv('data/titanic.csv')\n",
    "titanicY = titanicX.pop(\"Survived\")\n",
    "titanicX = normalize(titanicX)\n",
    "\n",
    "for i in range(2):\n",
    "    alpha = i\n",
    "    titanicLR = LogisticRegression(titanicX.columns, titanicY.unique(), alpha)\n",
    "    titanicLR.fit(titanicX, titanicY)\n",
    "    titanicPredictions = titanicLR.predict(titanicX)\n",
    "\n",
    "    print(\"==== Results for alpha =  \" + str(alpha) + \"====\")\n",
    "    print(\"Accuracy and Predictions for Logistic Regression: \")\n",
    "    print(\"Accuracy: %0.2f%%\" % (titanicLR.score(titanicX, titanicY)*100))\n",
    "    titanicLR.print_errors(titanicX, titanicY, titanicPredictions, 10)\n",
    "\n",
    "    print(\"Coefficients and intercept for Logistic Regression: \")\n",
    "    printWeights(titanicX.columns,  titanicLR.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to predict for multiple classes\n",
    "\n",
    "class LogisticRegression(LogisticRegression):\n",
    "    \n",
    "    def binary_fit(self, Xinput, Y):#binary version of fit, copied from previous predict function\n",
    "        \"\"\"\n",
    "        train model weights via gradient descent\n",
    "        \"\"\"\n",
    "        X = Xinput.copy()\n",
    "        X[\"bias\"] = 1#add a bias\n",
    "        self.weights = np.zeros(len(X.columns))\n",
    "        self.w = self.weights\n",
    "        for z in range(MAXITER):\n",
    "            gradient = np.zeros(len(X.columns))\n",
    "            for index, xi in X.iterrows(): #might be problem\n",
    "                p = self.loss(xi)\n",
    "                yi = self.labelToNumberDict[Y[index]]\n",
    "                error = p - yi\n",
    "\n",
    "                for j in range(len(X.columns)):\n",
    "                  gradient[j] = error*xi.iloc[j]\n",
    "                self.weights -= alpha*gradient\n",
    "    \n",
    "    def binary_predict(self, Xinput):#binary version of predict, copied from previous predict function\n",
    "        \"\"\"\n",
    "        classifies the examples in X.  returns the predicted label for each example in X.\n",
    "        \"\"\"\n",
    "        X = Xinput.copy()\n",
    "        X[\"bias\"] = 1#add a bias\n",
    "        predictions = []\n",
    "        for index, xi in X.iterrows():\n",
    "            predictions.append(self.labels[int(np.round(self.loss(xi)))])\n",
    "        return predictions\n",
    "    \n",
    "    def fit(self, Xinput, Y):\n",
    "        \"\"\"\n",
    "        multiclass version of the fit function that uses binary fit. \n",
    "        train a model for every class that can determine the probability that a given example belongs to a given class.\n",
    "        \"\"\"\n",
    "        self.classToModel = {}\n",
    "        for k in self.labels:\n",
    "            y_k = Y.copy()\n",
    "            for i in range(len(y_k)):\n",
    "                if y_k.iloc[i] == k:\n",
    "                    y_k.iloc[i] = 'Pos' #set to a positive 1\n",
    "                else:\n",
    "                    y_k.iloc[i] = 'Neg'\n",
    "            model_k = LogisticRegression(Xinput.columns, y_k.unique(), alpha)\n",
    "            model_k.binary_fit(Xinput, y_k)\n",
    "            self.classToModel[k] = model_k\n",
    "    \n",
    "    def predict(self, Xinput):#multiclass version of predict\n",
    "        \"\"\"\n",
    "        to classify an example, ask each model for the probability that the example belongs to its class,\n",
    "        and return the class with the highest probability\n",
    "        \"\"\"\n",
    "        final_predictions = []\n",
    "        for i in range(len(Xinput)):#examples in probabilities\n",
    "            exampleXi = Xinput.iloc[[i]] # example row as a dataframe\n",
    "            positive_prob_k = []#for each class, \n",
    "            for k in self.labels:#for each example, predict probability of it being a given model. \n",
    "                model_k = self.classToModel[k]\n",
    "                prob_k = model_k.predict_prob(exampleXi)#assuming that prediction and probability will coincide. \n",
    "                prediction = model_k.binary_predict(exampleXi)\n",
    "                if prediction[0] == 'Pos':#append chance that it is positive for a given class k\n",
    "                    positive_prob_k.append(max(prob_k[0]))# 'Pos' probability will be the largest one\n",
    "                else:\n",
    "                    positive_prob_k.append(min(prob_k[0]))# 'Neg' probability will be the largest one. \n",
    "            #use maximum probability to predict the example. \n",
    "            maxProbK = 0#find the class with the maximum probability of being positive. \n",
    "            maxK = self.labels[0] #must be at least 1 label\n",
    "            for kIndex in range(len(self.labels)):\n",
    "                currentProbK = positive_prob_k[kIndex]#probability that our one example is positive for this class k. \n",
    "                if currentProbK > maxProbK:\n",
    "                    maxProbK = currentProbK\n",
    "                    maxK = self.labels[kIndex]\n",
    "            final_predictions.append(maxK)\n",
    "        return final_predictions\n",
    "    \n",
    "    def score(self, X_test, y_test):#score by accuracy: # correct / total predictions\n",
    "        predictions = self.predict(X_test)\n",
    "        numCorrect = 0\n",
    "        for i in range(len(predictions)):\n",
    "            correct = y_test.iloc[i]\n",
    "            if predictions[i] == correct:\n",
    "                numCorrect += 1\n",
    "        return float(numCorrect)/float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "toy test\n",
      "type(toyX): <class 'pandas.core.frame.DataFrame'>\n",
      "type(toyX): <class 'pandas.core.frame.DataFrame'>\n",
      "multi logictic predicitons:\n",
      " ['Neg', 'Neg', 'Neg', 'Pos', 'Neg', 'Pos', 'Neg']\n",
      "toy labels: |    | stroke   |\n",
      "|---:|:---------|\n",
      "|  0 | Neg      |\n",
      "|  1 | Pos      |\n",
      "|  2 | Neg      |\n",
      "|  3 | Pos      |\n",
      "|  4 | Neg      |\n",
      "|  5 | Pos      |\n",
      "|  6 | Neg      |\n",
      "\n",
      "zoo test\n",
      "load data\n",
      "fitting\n",
      "done training\n",
      "accuracy should be 1.0: 1.0\n",
      "done training\n",
      "accuracy for train-test split: 0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "def toyTest():\n",
    "    print(\"\\ntoy test\")\n",
    "    mc = LogisticRegression(toyX.columns, toyY.unique(),alpha) \n",
    "    print(f\"type(toyX): {type(toyX)}\")\n",
    "    mc.fit(toyX,toyY)\n",
    "    print(f\"type(toyX): {type(toyX)}\")\n",
    "    predictions = mc.predict(toyX)\n",
    "    print(f\"multi logictic predicitons:\\n {predictions}\")\n",
    "    print(f\"toy labels: {toyY.to_markdown()}\")\n",
    "\n",
    "def digitsTest():\n",
    "    print(\"digits test\")\n",
    "    digits = datasets.load_digits(as_frame=True)\n",
    "    print(len(digits))\n",
    "    print(f\"digits type: {type(digits)}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.4, random_state=2)\n",
    "    digitsX = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
    "    digitsX['target'] = pd.Series(digits.target)\n",
    "    digitsY = digitsX.pop(\"target\")\n",
    "    print(\"train-test split finished\")\n",
    "    \n",
    "    mc = LogisticRegression(digitsX.columns, digitsY.unique(),alpha)\n",
    "    print(f\"type(X_train): {type(X_train)}\")\n",
    "    print(f\"digitsY: {digitsY}\")\n",
    "    print(f\"y_train: {y_train}\")\n",
    "    print(\"training...\")\n",
    "    mc.fit(digitsX, digitsY)\n",
    "    print(\"done training\")\n",
    "    print(f\"score: {mc.score(digitsX, digitsY)}\")\n",
    "\n",
    "def zooTest():\n",
    "    #this part works\n",
    "    print(\"\\nzoo test\")\n",
    "    X = pd.read_csv('data/zoo.csv')\n",
    "    Y = X.pop(\"animalType\")\n",
    "    print(\"load data\")\n",
    "    mc = LogisticRegression(X.columns, Y.unique(),alpha)\n",
    "    print(\"fitting\")\n",
    "    mc.fit(X, Y)#use whole training set\n",
    "    print(\"done training\")\n",
    "    print(f\"accuracy should be 1.0: {mc.score(X, Y)}\")\n",
    "    \n",
    "    #train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.copy(), Y.copy(), test_size=0.4, random_state=2)\n",
    "    mc.fit(X_train, y_train)\n",
    "    print(\"done training\")\n",
    "    print(f\"accuracy for train-test split: {mc.score(X_test, y_test)}\")\n",
    "\n",
    "toyTest()\n",
    "zooTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits test\n",
      "7\n",
      "digits type: <class 'sklearn.utils.Bunch'>\n",
      "train-test split finished\n",
      "type(X_train): <class 'pandas.core.frame.DataFrame'>\n",
      "digitsY: 0       0\n",
      "1       1\n",
      "2       2\n",
      "3       3\n",
      "4       4\n",
      "       ..\n",
      "1792    9\n",
      "1793    0\n",
      "1794    8\n",
      "1795    9\n",
      "1796    8\n",
      "Name: target, Length: 1797, dtype: int64\n",
      "y_train: 399     3\n",
      "28      8\n",
      "641     9\n",
      "821     3\n",
      "584     4\n",
      "       ..\n",
      "1558    3\n",
      "1608    6\n",
      "493     1\n",
      "527     1\n",
      "1192    5\n",
      "Name: target, Length: 1078, dtype: int64\n",
      "training...\n",
      "done training\n",
      "score: 0.9404563160823595\n"
     ]
    }
   ],
   "source": [
    "digitsTest()#takes about 10 minutes to finish running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
